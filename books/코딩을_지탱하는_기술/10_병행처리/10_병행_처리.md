# 병행 처리

## 병행 처리란?

한 번에 하나의 처리만 실행하는 것보다 복수 개의 처리를 동시에 실행하는 것이 좋다. 하지만 실행하기 위한 CPU가 하나 밖에 없는데 어떻게 복수의 처리를 동시에 실행할 수 있을까? 이는 엄청~ 짧은 순간에 복수의 처리를 **변경해가면서 실행(잘게 분할해서 실행)** 하기 떄문이다. 이것이 병행처리의 가장 중요한 개념이다.

편리한 병행처리를 실현하기 위해 **프로세스**나 **스레드** 등의 개념이 만들어졌다. 하지만 병행 처리로 인해 문제가 발생하는데 그 대안책으로 **락(Lock)** 또는 **파이버(fiber)** 등의 개념이 나온다. 

여기서 **병행성(Concurrency, 동시성)** 과 **병렬성(Parallelism)** 이 혼동될 수 있는데 프로그래밍언어에 있어서의 병행성과 하드웨어의 병렬성은 상호간에 독립된 개념이다. 병렬성은 하드웨어 측면의 개념이다. 이 장에서 설명하는 것은 프로그래밍 측면의 병행성이다. 주안점이 되는 것은 프로세스나 스레드 등의 개념이다.

## 처리를 변경하는 2가지 방법
하나의 실행 회로(CPU)를 사용해서 복수의 처리를 실행하기위해서는 **'언제 교대할 것인가?'** 를 정해야한다. 정하는 방법은 크게 2가지로 나눌 수 있다.

### 협력적 멀티태스크(multi-task, 병행처리)
- '타이밍이 좋은 시점에서 교대'하는 방법
- 처리가 일단락되는 시점에 **자발적**으로 교대하는 방법
- 어떤 처리가 '교대하자'라고 말하지 않고 계속 실행되면 다른 처리는 계속 기다려야 하는 문제점이 있음
- '모든 처리가 최적의 간격으로 교대한다'는 신뢰 관계를 기반으로 성립하는 시스템

> 협력적 대신 비선점(non-preemptive)이라고도 말하는 것 같음.

### 선점적(preemptive) 멀티태스크
- 일정 시간에 교대 
- 개별 프로그램과 다른 프로그램(**태스크 스케줄러**)가 존재
- 실행되고 있는 처리를 **강제적**으로 중단하고 다른 처리가 실행될 수 있도록 함
- 선점적이란 '타인의 행동을 막기 위한'이란 의미

## Race condition 방지법
언제 '교대해'라는 명령이 떨어질지 모르는 상황에서 제대로 동작하는 프로그램을 만드는 것은 어려운 문제이다. 

다음 예시를 보자.

~~~
if 예금 잔고 >= 10000 {
    예금 잔고 -= 10000;
    10000 출금;
}
~~~

'예금 잔고'라는 변수를 공유하고 있는 상황에서 프로그램이 **여러개** 실행되었을때 문제가 발생한다.

~~~
(예금잔고 = 15000, 프로그램 A 실행)
A: if 예금 잔고 >= 10000 -> True
(여기서 B 프로그램으로 교대)
B: if 예금 잔고 >= 10000 -> True
B: 예금 잔고 -= 10000; 10000 출금
(예금 잔고가 5000이 되고 다시 A 프로그램으로 교대)
A: 예금 잔고 -= 10000; 10000 출금
(예금 잔고가 5000 밖에 없지만 10000이 출금됨) -> Problem!
~~~

여러개의 프로그램이 공유하고 있는 변수를 서로 접근하려고 해서 발생하는 현상인데 이를 **Race condition** 이라고 하며 이 프로그램을 '스레드 세이프(thread-safe)하지 않다.'라고 한다.

### Race condition의 3가지 조건
Race condition이 발생하기 위해서는 다음 3가지 조건은 모두 만족해야 발생한다.

1. 2가지 처리가 변수를 공유하고 있음
2. 적어도 하나의 처리가 그 변수를 변경함
3. 한쪽 처리가 한 단락 마무리 되기 전에, 다른 한쪽의 처리가 끼어들 가능성이 있음

역으로 말하면, 3가지 조건중 하나라도 성립되지 않으면 Race condition이 발생하지 않는다. 즉, 병행 실행 시에도 안정된 프로그램을 만들 수 있다. 3가지 조건 중 하나라도 만족시키지 않기위해 어떠한 방법들이 있는지 알아보자.

#### 1. 공유하지 않는다.

처음부터 아무것도 공유하지 않으면 **1번**은 발생하지 않기 때문에 Race condition을 신경 쓸 필요가 없다.

##### 1-1. 프로세스에서는 메모리를 공유하지 않는다.
UNIX에서는 실행 중의 프로그램을 프로세스라고 부르는데 **서로 다른 프로세스는 메모리를 공유하지 않는다.** DB 접속, 파일 read/write 등 무엇인가를 공유했을 때만 주의하면 된다.

옛날 프로세스는 메모리를 공유하여 지금의 스레드와 비슷하다. 하지만 너무 많은 기능을 넣으려고 해서 시스템이 복잡해지고 보다 간단한 시스템을 만들자는 움직임이 있었는데 그 결과 1970년 OS 'UNICS'가 만들어지고 이것이 현재의 UNIX이다.

UNIX에서는 프로세스 별로 '사용해도 좋은 메모리 영역'을 결정하여 '다른 프로세스와 메모리를 공유하지 않는다'를 실현하는 구조를 채용하였다.

> 자세히 알고 싶으면 가상 주소 공간(virtual memory 와 다름)에 대해 알아보자 

UNIX는 '하나의 프로세스 안에서 병행해서 실행되는 처리는 하나'라는 구조였다. 즉, 복수의 처리를 병행해서 실행하고 싶으면 복수의 프로세스를 구동해야한다. 메모리를 공유하지 않는 것은 너무 엄격한 구조였다. 이러한 접근법은 실패하고 UNIX를 출시하고 약 10년 후 메모리를 공유하는 **'경량 프로세스'** 가 만들어지는데 이것이 나중에 **'스레드'** 라고 불리게 되었다. 

##### 1-2. 액터 모델 (Actor model)

'메모리를 공유하지 않는다'는 설계 방침에서의 또 다른 흐름이 **액터 모델**이다.

액터 모델은 병행해서 동작하는 복수의 처리들의 정보 교환 방법으로 '메모리를 공유한다'가 아닌 **'메세지를 보낸다'** 를 제안한다. 

처리는 비동기로 이루어진다.

#### 2. 변경하지 않는다 - Immutable
**'메모리를 공유해도 변경하지 않으면 문제없다'** 는 **2번**에 대한 대응책도 있다.

예를 들어 C++의 `const`, Scala에서 `val` 등이 있다.
 
#### 3. 끼어들지 않는다
**3번**을 방지하기 위해서 다음 2가지 방법이 있다.

##### 3-1. 협력적 스레드 사용 - 파이버, 코루틴, 그린 스레드
파이버(fiber), 코루틴(coroutine), 그린 스레드(Green thread)등으로 불리는 기법을 사용하는 것인데 이는 스레드가 선점성을 가지고 끼어드는 원인 때문에 **협력적 스레드**를 만들면 된다는 생각이다. 

물론 협력적 멀티태스크이기 때문에 어떤 스레드가 CPU를 독잡하면 다른 스레드는 멈춘다. 어디까지나 각 스레드가 협력적으로 최적의 순간을 맞춘다는 사실을 전제로 하고 있다.

##### 3-2. 끼어들면 곤란해지는 처리에 표식을 남김 - 락, 뮤텍스, 세마포어

다른 방법으로는 '지금 끼어들면 곤란해'라는 **표식**을 공유하는 것이다. 

이 방법에는 **락(Lock)**, **뮤텍스(Mutex)**, **세마포어(Semaphore)** 등 여러가지 기법이 있는데 핵심 개념은 '사용중'이라는 표식을 공유하는 것이다. 

> **NOTE** 이후 락(Lock)이라고 명칭을 통일하도록 한다

'사용중'이라는 표식을 붙여둘 뿐, 표식을 확인하지 않고 실행하는 스레드가 있으면 아무 소용 없다. 처리 흐름의 일부만을 협력적으로 양보하는 구조라고 볼 수 있다.

## 락의 문제점과 해결책

### 교착 상태(Dead lock) 발생

### 합성할 수 없다

### 트랜잭션 메모리
